{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etape 1 : Extraction de la dixième colonne et des 1 000 premières réponses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des bibliothèques et des fonctions\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import re\n",
    "\n",
    "#compteur\n",
    "cpt=0\n",
    "\n",
    "#déclaration des variables\n",
    "rep=()\n",
    "\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "\n",
    "#gérer les erreurs de noms de répertoire\n",
    "try:\n",
    "#chemin répertoires\n",
    "    rep_in = \"projet_tal_rep_in/\" \n",
    "    rep_out = \"projet_tal_rep_out/\"\n",
    "except FileNotFoundError:\n",
    "    print (\"Les chemins des répertoires ou leurs noms ne sont pas corrects\")\n",
    "else:\n",
    "#lire tous les fichiers du répértoire\n",
    "    for fichier_in in os.listdir(rep_in):\n",
    "        if fichier_in.endswith(\".csv\"):#pour ne traiter que les fichiers qui se terminent par .csv\n",
    "            fichier_out = re.sub(\"\\.csv\",\"_out.csv\",fichier_in)\n",
    "        \n",
    "#ouverture des fichiers csv \n",
    "            reader= csv.reader(codecs.open(rep_in+fichier_in,\"r\",encoding=\"utf-8\"),delimiter=\",\")#paramétrage du séparateur de colonnes\n",
    "            writer= csv.writer(codecs.open(rep_out+fichier_out,\"w\",encoding=\"utf-8\"),delimiter=\"\\t\")\n",
    "\n",
    "#pour ne garder que la colonne K\n",
    "            next(reader)#pour ne pas analyser la première ligne car elle contient la question\n",
    "            for row in reader:\n",
    "#un compteur pour ne traiter que les 1 000 premiers messages\n",
    "                if cpt<1001:\n",
    "                    rep=row[11]\n",
    "                    cpt+=1\n",
    "                else:\n",
    "                    break\n",
    "                if not rep==\"\": #pour ne pas garder les lignes vides\n",
    "                    writer.writerow([row[11]])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etape 2 : Nettoyage du fichier (tokenisation, suppression des accents et des cédilles, lemmatisation, mise en minuscule, suppression des mots vides, extraction des noms et adjectifs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des bibliothèques et des fonctions\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences \n",
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "import re\n",
    "\n",
    "#choisir le nombre de colonne souhaité\n",
    "col_pos = True\n",
    "col_lemma = True\n",
    "col_noStopWord = True\n",
    "\n",
    "#gérer les erreurs de noms de répertoire\n",
    "try:\n",
    "#chemin répertoire\n",
    "    rep_out2=\"projet_tal_rep_out2/\"\n",
    "except FileNotFoundError:\n",
    "    print (\"Le chemin du répertoire ou son nom n'est pas correct\")\n",
    "else:          \n",
    "    for fichier_in2 in os.listdir(rep_out):\n",
    "        if fichier_in2.endswith(\".csv\"):\n",
    "            fichier_out2 = re.sub(\"\\.csv\",\"2.csv\",fichier_in2)\n",
    "#ouverture fichiers csv \n",
    "            reader= csv.reader(codecs.open(rep_out+fichier_in2,\"r\",encoding=\"utf-8\"),delimiter=\"\\t\")\n",
    "            writer= csv.writer(codecs.open(rep_out2+fichier_out2,\"w\",encoding=\"utf-8\"),delimiter=\"\\t\")\n",
    "        \n",
    "            for row in reader:\n",
    "                doc = nlp(row[0])\n",
    "                rep_lemme=\"\"#pour récupérer la réponses avec les mots lemmatisés + en minuscule\n",
    "                rep_noStopw=\"\"#pour récupérer la réponse sans les mots-vides\n",
    "                rep_pos=\"\"#pour récupérer les groupes nominaux (noms et adjectifs)\n",
    "                \n",
    "                #suppression des accents et des cédilles\n",
    "                row[0]=re.sub(r\"é\",\"e\",row[0])\n",
    "                row[0]=re.sub(r\"è\",\"e\",row[0])\n",
    "                row[0]=re.sub(r\"ê\",\"e\",row[0])\n",
    "                row[0]=re.sub(r\"à\",\"a\",row[0])\n",
    "                row[0]=re.sub(r\"â\",\"a\",row[0])\n",
    "                row[0]=re.sub(r\"î\",\"i\",row[0])\n",
    "                row[0]=re.sub(r\"ï\",\"i\",row[0])\n",
    "                row[0]=re.sub(r\"ë\",\"e\",row[0])\n",
    "                row[0]=re.sub(r\"ô\",\"o\",row[0])\n",
    "                row[0]=re.sub(r\"ü\",\"u\",row[0])\n",
    "                row[0]=re.sub(r\"û\",\"u\",row[0])\n",
    "                row[0]=re.sub(r\"ù\",\"u\",row[0])\n",
    "                row[0]=re.sub(r\"ç\",\"c\",row[0])\n",
    "                \n",
    "                doc = nlp(row[0])\n",
    "                \n",
    "                for token in doc:\n",
    "                    if col_lemma:\n",
    "#lemmatisation + mise minuscule\n",
    "                        rep_lemme+=token.lemma_.lower()+\" \"\n",
    "#enlever les mots-vides\n",
    "                        if col_noStopWord:\n",
    "                            if nlp.vocab[token.text].is_stop==False:\n",
    "                                rep_noStopw+=token.lemma_+\" \"\n",
    "#extraire les groupes nominaux (NOUN et ADJ)\n",
    "                                if col_pos:\n",
    "                                    if token.pos_ == \"NOUN\" or token.pos_==\"ADJ\":\n",
    "                                        rep_pos+=token.lemma_.lower()+\" \"\n",
    "                \n",
    "              \n",
    "                writer.writerow([row[0],rep_lemme,rep_noStopw,rep_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etape 3 : Calcule du tf-idf et clustering avec l'algorithme K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0:\n",
      " produit\n",
      " pesticide\n",
      " polluant\n",
      " agriculture\n",
      " entreprise\n",
      " consommation\n",
      " pollution\n",
      " industriel\n",
      " pays\n",
      " taxe\n",
      "\n",
      "Cluster 1:\n",
      " vehicule\n",
      " electrique\n",
      " niveau\n",
      " voiture\n",
      " polluant\n",
      " mondial\n",
      " diesel\n",
      " entreprise\n",
      " pollution\n",
      " pourcent\n",
      "\n",
      "Cluster 2:\n",
      " pollueur\n",
      " pays\n",
      " gros\n",
      " payeur\n",
      " avion\n",
      " grand\n",
      " sanction\n",
      " politique\n",
      " etat\n",
      " paquebot\n",
      "\n",
      "Cluster 3:\n",
      " energie\n",
      " renouvelable\n",
      " fossile\n",
      " nucleaire\n",
      " centrale\n",
      " propre\n",
      " solaire\n",
      " reduire\n",
      " consommation\n",
      " production\n",
      "\n",
      "Cluster 4:\n",
      " transport\n",
      " commun\n",
      " ville\n",
      " voiture\n",
      " camion\n",
      " grand\n",
      " produit\n",
      " routier\n",
      " deplacement\n",
      " public\n"
     ]
    }
   ],
   "source": [
    "#import des bibliothèques et des fonctions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "import csv\n",
    "\n",
    "#gérer les erreurs de noms de répertoire\n",
    "try:\n",
    "#chemin répertoire\n",
    "    rep_out3=\"projet_tal_rep_out3/\"\n",
    "except FileNotFoundError:\n",
    "    print (\"Le chemin du répertoire ou son nom n'est pas correct\")\n",
    "else:\n",
    "    for fichier_in3 in os.listdir(rep_out2):\n",
    "        if fichier_in3.endswith(\".csv\"):\n",
    "            fichier_out3 = re.sub(\"2\\.csv\",\"3.txt\",fichier_in3)#pour avoir des fichiers .txt en sortie\n",
    "#ouverture fichiers csv \n",
    "            reader= csv.reader(codecs.open(rep_out2+fichier_in3,\"r\",encoding=\"utf-8\"),delimiter=\"\\t\")#paramétrage du séparateur de colonnes\n",
    "            writer=codecs.open(rep_out3+fichier_out3,\"w\", encoding=\"utf-8\")\n",
    "       \n",
    "            content = []#une liste pour récupérer chaque réponse\n",
    "            for row in reader:\n",
    "                content.append(row[3])#ne récupère que la 4e colonne du document\n",
    "        \n",
    "#calcule le tf-idf\n",
    "            tfidf = TfidfVectorizer(ngram_range=(1,1),max_features = 8000,token_pattern=r\"(?u)\\S\\S+\")#token_pattre = découpe au niveau des espaces\n",
    "            textV=tfidf.fit(content)\n",
    "            textV = tfidf.transform(content)\n",
    "        \n",
    "        \n",
    "#modèle K-means\n",
    "            model = KMeans(n_clusters=5, init='k-means++', max_iter=500, n_init=20)#charger le modèle de k-means\n",
    "            model.fit(textV)\n",
    "\n",
    "            prediction = model.fit_predict(textV)\n",
    "            #print(\"\\nVoici les principaux termes en fonction des clusters :\")\n",
    "            writer.write(\"Cluster\\tRéponses d'internautes\\n\")\n",
    "            for i in range(0, len(prediction)):\n",
    "                writer.write(str(prediction[i])+\"\\t\"+content[i]+\"\\n\")\n",
    "                #print (str(prediction[i])+\" \"+content[i])\n",
    "            \n",
    "#liste les mots de chaque clusters\n",
    "            #print(\"\\nVoici les principaux termes en fonction des clusters :\")\n",
    "            writer.write(\"\\nClusters\\tPrincipaux termes en fonction des clusters :\\n\")\n",
    "            order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "            terms = tfidf.get_feature_names()\n",
    "            for i in range(5):#choisir le nombre de clusters à afficher\n",
    "                writer.write(\"\\nCluster %d:\" % i)\n",
    "                print(\"\\nCluster %d:\" % i),\n",
    "                for ind in order_centroids[i, :10]:#choisir le nombre de mots-clés à afficher\n",
    "                    print(' %s' % terms[ind]),\n",
    "                    writer.write('\\t %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le rappel est de 0.75\n",
      "\n",
      "La précision est de 0.7241379310344828\n",
      "\n",
      "La f-mesure est de  0.736842105263158\n"
     ]
    }
   ],
   "source": [
    "#import des bibliothèques et des fonctions\n",
    "import codecs\n",
    "\n",
    "#gérer les erreurs de noms de fichiers\n",
    "try:\n",
    "#chemin fichiers\n",
    "    fichier_in3=codecs.open(\"LA_TRANSITION_ECOLOGIQUE_COPIE.txt\",\"r\",encoding=\"utf-8\")\n",
    "    fichier_out=codecs.open(\"LA_TRANSITION_ECOLOGIQUE_EVAL.txt\",\"w\",encoding=\"utf-8\")\n",
    "except NameError:\n",
    "    print (\"Le nom du fichier n'est pas correct\")\n",
    "else:\n",
    "    fichier_out.write(\"Voici les résultats de l'évaluation des clusters :\\n\")\n",
    "\n",
    "#calcule du rappel\n",
    "    def rappel(mot_0,mot_1,mot_2,mot_3,mot_4,fichier):\n",
    "        sentence=[]\n",
    "        vp=0\n",
    "        fn=0\n",
    "        for ligne in fichier_in3:\n",
    "            if len(ligne) > 1:\n",
    "                ligne=ligne.split(\"\\t\")\n",
    "                if mot_0 in ligne[0] and mot_0 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_1 in ligne[0] and mot_1 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_2 in ligne[0] and mot_2 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_3 in ligne[0] and mot_3 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_4 in ligne[0] and mot_4 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_0 in ligne[0] and not mot_0 in ligne[1]:\n",
    "                    fn+=1\n",
    "                elif mot_1 in ligne[0] and not mot_1 in ligne[1]:\n",
    "                    fn+=1\n",
    "                elif mot_2 in ligne[0] and not mot_2 in ligne[1]:\n",
    "                    fn+=1\n",
    "                elif mot_3 in ligne[0] and not mot_3 in ligne[1]:\n",
    "                    fn+=1\n",
    "                elif mot_4 in ligne[0] and not mot_4 in ligne[1]:\n",
    "                    fn+=1\n",
    "        rep_rap=vp/(vp+fn)\n",
    "        return rep_rap\n",
    "    \n",
    "    rep_rappel = rappel(mot_0=\"0\",mot_1=\"1\", mot_2=\"2\",mot_3=\"3\", mot_4=\"4\",fichier=fichier_in3)\n",
    "    fichier_out.write(\"Le rappel est de \"+str(rep_rappel)+\"\\n\")\n",
    "    print(\"Le rappel est de \"+str(rep_rappel)+\"\\n\")\n",
    "    \n",
    "    fichier_in3=codecs.open(\"LA_TRANSITION_ECOLOGIQUE_COPIE.txt\",\"r\",encoding=\"utf-8\")\n",
    "\n",
    "#calcule de la précision\n",
    "    def precision(mot_0,mot_1,mot_2,mot_3,mot_4,fichier):\n",
    "        sentence=[]\n",
    "        vp=0\n",
    "        fp=0\n",
    "        for ligne in fichier_in3:\n",
    "            if len(ligne) > 1:\n",
    "                ligne=ligne.split(\"\\t\")\n",
    "                if mot_0 in ligne[0] and mot_0 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_1 in ligne[0] and mot_1 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_2 in ligne[0] and mot_2 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_3 in ligne[0] and mot_3 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_4 in ligne[0] and mot_4 in ligne[1]:\n",
    "                    vp+=1\n",
    "                elif mot_0 in ligne[1] and not mot_0 in ligne[0]:\n",
    "                    fp+=1\n",
    "                elif mot_1 in ligne[1] and not mot_1 in ligne[0]:\n",
    "                    fp+=1\n",
    "                elif mot_2 in ligne[1] and not mot_2 in ligne[0]:\n",
    "                    fp+=1\n",
    "                elif mot_3 in ligne[1] and not mot_3 in ligne[0]:\n",
    "                    fp+=1\n",
    "                elif mot_4 in ligne[1] and not mot_4 in ligne[0]:\n",
    "                    fp+=1\n",
    " \n",
    "        rep_pre=vp/(vp+fp)\n",
    "        return rep_pre\n",
    "    \n",
    "    rep_precision = precision(mot_0=\"0\",mot_1=\"1\", mot_2=\"2\",mot_3=\"3\", mot_4=\"4\",fichier=fichier_in3)\n",
    "    fichier_out.write(\"La précision est de \"+str(rep_precision)+\"\\n\")\n",
    "    print(\"La précision est de \"+str(rep_precision)+\"\\n\")\n",
    "    \n",
    "#calcule de la f-mesure\n",
    "    def fmesure(fichier,rappel,precision):\n",
    "        rep=(2*rappel*precision)/(rappel+precision)\n",
    "        print(\"La f-mesure est de \",rep)\n",
    "        fichier_out.write(\"la f-mesure est de \"+str(rep))\n",
    "    \n",
    "    fmesure(fichier=fichier_in3,rappel=rep_rappel,precision=rep_precision)\n",
    "\n",
    "    fichier_in3.close()\n",
    "    fichier_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
